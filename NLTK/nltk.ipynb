{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Welcome, to learning NLP tutorials. Please watch entire course! to become expert in NLP.\n"
     ]
    }
   ],
   "source": [
    "corpus=\"\"\"Hello Welcome, to learning NLP tutorials. Please watch entire course! to become expert in NLP.\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Dell/nltk_data', 'd:\\\\learn1\\\\myenv\\\\nltk_data', 'd:\\\\learn1\\\\myenv\\\\share\\\\nltk_data', 'd:\\\\learn1\\\\myenv\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', 'path_to_your_nltk_data_directory', 'path_to_your_nltk_data_directory']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "print(nltk.data.path)\n",
    "nltk.data.path.append('path_to_your_nltk_data_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello world.', ' This is a test sentence!', 'show next.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import regexp_tokenize\n",
    "\n",
    "corpus = \"Hello world. This is a test sentence!show next.\"\n",
    "sentences = regexp_tokenize(corpus, pattern=r'\\s*[^.!?]*[.!?]')\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'This', 'is', 'a', 'test', 'sentence', 'show', 'next']\n"
     ]
    }
   ],
   "source": [
    "##paragraph->words\n",
    "words = regexp_tokenize(corpus, pattern=r'\\w+')\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world']\n",
      "['This', 'is', 'a', 'test', 'sentence']\n",
      "['show', 'next']\n"
     ]
    }
   ],
   "source": [
    "for sent in sentences:\n",
    "    word=regexp_tokenize(sent,pattern=r'\\w+')\n",
    "    print(word)\n",
    "# Use regexp_tokenize to tokenize the text into words and punctuation\n",
    "tokens = regexp_tokenize(corpus, pattern=r'\\w+|[^\\w\\s]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eaten->eaten\n",
      "eat->eat\n",
      "eating->eat\n",
      "writes->write\n",
      "programs->program\n",
      "programming->program\n",
      "finally->final\n",
      "finalize->final\n",
      "history->histori\n"
     ]
    }
   ],
   "source": [
    "##stemming\n",
    "import nltk.stem\n",
    "\n",
    "words=['eaten','eat','eating','writes','programs','programming','finally','finalize','history']\n",
    "##porter stem\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming=PorterStemmer()\n",
    "for word in words:\n",
    "    print(word+'->'+stemming.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##RegexpStemmer class\n",
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|able$|e$',min=4)\n",
    "print(reg_stemmer.stem('eating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eaten->eaten\n",
      "eat->eat\n",
      "eating->eat\n",
      "writes->write\n",
      "programs->program\n",
      "programming->program\n",
      "finally->final\n",
      "finalize->final\n",
      "history->histori\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##snowball stemmer-better than porter stemmer(accuracy)\n",
    "from nltk.stem import SnowballStemmer\n",
    "sn_stemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+'->'+sn_stemmer.stem(word))\n",
    "sn_stemmer.stem('fairly'),sn_stemmer.stem('sportingly')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "eaten->eat\n",
      "eat->eat\n",
      "eating->eat\n",
      "writes->write\n",
      "programs->program\n",
      "programming->program\n",
      "finally->finally\n",
      "finalize->finalize\n",
      "history->history\n"
     ]
    }
   ],
   "source": [
    "##Wordnet Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "''' \n",
    "pos->Noun-n,verb-v,adjective-a,adverb-r, default-n\n",
    "'''\n",
    "print(lemmatizer.lemmatize(\"going\",pos='v'))\n",
    "for word in words:\n",
    "    print(word+'->'+lemmatizer.lemmatize(word,pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "##StopWords\n",
    "paragraph=\"\"\"Today, I am here to deliver a speech on Dr APJ Abdul Kalam. APJ Abdul Kalam was born to Jainulabdeen and Ashiamma on October 15, 1931.\n",
    " His father was a boat owner and his mother was a homemaker. His family's economic situation was not strong, so at an early age, \n",
    " he began helping his family financially.\n",
    "He graduated in 1955 from the Madras Institute of Technology and graduated from St. Joseph's College, Tiruchirappalli, in Aerospace Engineering. \n",
    "He joined the Defense Research and Development Organization's (DRDO) Aeronautical Development Base as a Chief Scientist after his graduation. \n",
    "He won credit as Project Director-General for making India's first indigenous satellite (SLV III) rocket. \n",
    "It was his ultimate support that brought nuclear power to India. In July 1992, he was appointed Scientific Advisor to the Indian Ministry of Defence. \n",
    "As a national counsellor, he played a significant role in the world-famous nuclear tests at Pokhran II.\n",
    " In 1981, he was awarded the Padma Bhushan Award, in 1909 the Padma Vibhushan, and in 1997 the highest civilian award of India' Bharat Ratna \n",
    " 'for modernizing the defence technology of India and his outstanding contribution. \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today i deliver speech dr apj abdul kalam',\n",
       " 'apj abdul kalam bear jainulabdeen ashiamma october 15 1931',\n",
       " 'his father boat owner mother homemaker',\n",
       " 'his family economic situation strong early age begin help family financially',\n",
       " 'he graduate 1955 madras institute technology graduate st',\n",
       " 'joseph college tiruchirappalli aerospace engineer',\n",
       " 'he join defense research development organization drdo aeronautical development base chief scientist graduation',\n",
       " 'he credit project director general make india first indigenous satellite slv iii rocket',\n",
       " 'it ultimate support bring nuclear power india',\n",
       " 'in july 1992 appoint scientific advisor indian ministry defence',\n",
       " 'as national counsellor play significant role world famous nuclear test pokhran ii',\n",
       " 'in 1981 award padma bhushan award 1909 padma vibhushan 1997 highest civilian award india bharat ratna modernize defence technology india outstanding contribution']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = regexp_tokenize(paragraph, pattern=r'\\s*[^.!?]*[.!?]')\n",
    "for i in range(len(sentences)):\n",
    "    words=regexp_tokenize(sentences[i],pattern=r'\\w+')\n",
    "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words) #convert word to sentences\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
